{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment we will illustrate a very severe data leakage, that can often be found in competitions, where the pairs of object should be scored, e.g. predict $1$ if two objects belong to the same class and $0$ otherwise. \n",
    "\n",
    "The data in this assignment is taken from a real competition, and the funniest thing is that *we will not use training set at all* and achieve almost 100% accuracy score! We will just exploit the leakage.\n",
    "\n",
    "Now go through the notebook and complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import scipy.sparse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the test data. Note, that we don't have any training data here, just test data. Moreover, *we will not even use any features* of test objects. All we need to solve this task is the file with the indices for the pairs, that we need to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data with test indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairId</th>\n",
       "      <th>FirstId</th>\n",
       "      <th>SecondId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1427</td>\n",
       "      <td>8053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17044</td>\n",
       "      <td>7681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19237</td>\n",
       "      <td>20966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8005</td>\n",
       "      <td>20765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>16837</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3657</td>\n",
       "      <td>12504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2836</td>\n",
       "      <td>7582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>6136</td>\n",
       "      <td>6111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>23295</td>\n",
       "      <td>9817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>6621</td>\n",
       "      <td>7672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pairId  FirstId  SecondId\n",
       "0       0     1427      8053\n",
       "1       1    17044      7681\n",
       "2       2    19237     20966\n",
       "3       3     8005     20765\n",
       "4       4    16837       599\n",
       "5       5     3657     12504\n",
       "6       6     2836      7582\n",
       "7       7     6136      6111\n",
       "8       8    23295      9817\n",
       "9       9     6621      7672"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../readonly/data_leakages_data/test_pairs.csv')\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can think that there is a test dataset of images, and each image is assigned a unique `Id` from $0$ to $N-1$ (N -- is the number of images). In the dataframe from above `FirstId` and `SecondId` point to these `Id`'s and define pairs, that we should compare: e.g. do both images in the pair belong to the same class or not. So, for example for the first row: if images with `Id=1427` and `Id=8053` belong to the same class, we should predict $1$, and $0$ otherwise. \n",
    "\n",
    "But in our case we don't really care about the images, and how exactly we compare the images (as long as comparator is binary).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We suggest you to try to solve the puzzle yourself first.** You need to submit a `.csv` file with columns `pairId` and `Prediction` to the grader. The number of submissions allowed is made pretty huge to let you explore the data without worries. The returned score should be very close to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you do not want to think much** -- scroll down and follow the instructions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 368550 entries, 0 to 368549\n",
      "Data columns (total 3 columns):\n",
      "pairId      368550 non-null int64\n",
      "FirstId     368550 non-null int64\n",
      "SecondId    368550 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 8.4 MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often the amounts of FirstIds are used are used\n",
      "i.e. 1755 times the same FirstId is reused 15 times\n",
      "--------------------------------------------------\n",
      "Indices: Int64Index([15, 14, 13, 12, 11, 10, 9, 8, 7, 21, 20, 19, 18, 17, 16], dtype='int64')\n",
      "Unique values: 26325\n",
      "Unique values double-check: 26325\n",
      "Unique values triple-check: 26325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15    1755\n",
       "14    1755\n",
       "13    1755\n",
       "12    1755\n",
       "11    1755\n",
       "10    1755\n",
       "9     1755\n",
       "8     1755\n",
       "7     1755\n",
       "21    1755\n",
       "20    1755\n",
       "19    1755\n",
       "18    1755\n",
       "17    1755\n",
       "16    1755\n",
       "Name: FirstId, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It seems like Ids are reused\n",
    "print('How often the amounts of FirstIds are used are used')\n",
    "print('i.e. 1755 times the same FirstId is reused 15 times')\n",
    "print('-'*50)\n",
    "print('Indices:', test.FirstId.value_counts().value_counts().index)\n",
    "print('Unique values:', sum(test.FirstId.value_counts().value_counts()))\n",
    "print('Unique values double-check:', test.FirstId.nunique())\n",
    "print('Unique values triple-check:', 15*1755) # As 15 rows & each time 1755 values\n",
    "test.FirstId.value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often the amounts of SecondIds are used are used\n",
      "i.e. 1755 times the same SecondId is reused 14 times\n",
      "i.e. There is only one SecondId which is reused 23 times\n",
      "--------------------------------------------------\n",
      "Indices: Int64Index([14, 13, 12, 11, 10,  9,  8, 15, 17, 18, 19, 16, 20, 21,  7, 22,  6,\n",
      "             5,  4,  3,  2,  1, 23],\n",
      "           dtype='int64')\n",
      "Unique values: 26310\n",
      "Unique values double-check: 26310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14    1755\n",
       "13    1755\n",
       "12    1755\n",
       "11    1755\n",
       "10    1755\n",
       "9     1755\n",
       "8     1754\n",
       "15    1740\n",
       "17    1740\n",
       "18    1740\n",
       "19    1740\n",
       "16    1740\n",
       "20    1739\n",
       "21    1695\n",
       "7     1605\n",
       "22     150\n",
       "6       60\n",
       "5       16\n",
       "4       15\n",
       "3       15\n",
       "2       15\n",
       "1       15\n",
       "23       1\n",
       "Name: SecondId, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('How often the amounts of SecondIds are used are used')\n",
    "print('i.e. 1755 times the same SecondId is reused 14 times')\n",
    "print('i.e. There is only one SecondId which is reused 23 times')\n",
    "print('-'*50)\n",
    "print('Indices:', test.SecondId.value_counts().value_counts().index)\n",
    "print('Unique values:', sum(test.SecondId.value_counts().value_counts()))\n",
    "print('Unique values double-check:', test.SecondId.nunique())\n",
    "test.SecondId.value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we look closely, we see that we cann add them all up to 15 chunks of 1755:\n",
      "14    1755\n",
      "13    1755\n",
      "12    1755\n",
      "11    1755\n",
      "10    1755\n",
      "9     1755\n",
      "Name: SecondId, dtype: int64\n",
      "8 + 23: 1755\n",
      "15 + 1 1755\n",
      "16 + 2 1755\n",
      "17 + 3 1755\n",
      "18 + 4 1755\n",
      "20 + 5: 1755\n",
      "21 + 6: 1755\n",
      "7 + 22: 1755\n",
      "All we are left with is No 19, with 1740 values exactly the 15 missing from 26325 - 26310\n"
     ]
    }
   ],
   "source": [
    "print('If we look closely, we see that we cann add them all up to 15 chunks of 1755:')\n",
    "count_series = test.SecondId.value_counts().value_counts()\n",
    "print(count_series[:6])\n",
    "print('8 + 23:', count_series.iloc[6] + count_series.iloc[-1])\n",
    "#print(count_series.sort_index())\n",
    "for i in range(14, 18, 1):\n",
    "    print((i+1), '+', (i-13), count_series.sort_index().iloc[i] +  count_series.sort_index().iloc[i-14])\n",
    "print('20 + 5:', count_series.iloc[12] + count_series.iloc[17])    \n",
    "print('21 + 6:', count_series.iloc[13] + count_series.iloc[16])    \n",
    "print('7 + 22:', count_series.iloc[14] + count_series.iloc[15])\n",
    "print('All we are left with is No 19, with 1740 values exactly the 15 missing from 26325 - 26310')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current hypothesis: \t Assumming that each Id can only have one possible partner, each Id does have a partner, we have about the same amount of unique values in FirstId & SecondId, Ids overlap between First & Second (?) -> there can be only one combination where each Id is being used\n"
     ]
    }
   ],
   "source": [
    "print('Current hypothesis: \\t Assumming that each Id can only have one possible partner, each Id does have a partner, \\\n",
    "we have about the same amount of unique values in FirstId & SecondId, Ids overlap between First & Second (?) \\\n",
    "-> there can be only one combination where each Id is being used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737100\n",
      "If nunique is smaller than 26325 + 26310, Ids are reused:\n",
      "Unique values in concat: 26325\n"
     ]
    }
   ],
   "source": [
    "# Are Ids reused in First & Second ?\n",
    "concat = pd.concat([test['FirstId'], test['SecondId']])\n",
    "print(len(concat))\n",
    "print('If nunique is smaller than 26325 + 26310, Ids are reused:')\n",
    "print('Unique values in concat:', concat.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recap:\n",
    "# We have 26,325 unique IDs, hence a maximum of 13,162 pairs. (In fact only 13155, as only 26310 unique IDs in Second)\n",
    "# The IDs are duplicated at least 7x  and up to 21x in column FirstId & \n",
    "#                       at least 1x and up to 23x in column SecondId\n",
    "# To pair each unique ID in each possible combination the organizers would have needed (26,325 * 26,324) / 2 rows\n",
    "# (assuming that order does not matter) -> 346,489,650 ; Instead we only have 368,550 rows\n",
    "# Hence IDs cannot \n",
    "\n",
    "# Hypothesis: I think when creating the dataset they started with Putting each Id & its correct partner for a total\n",
    "# of 13155 rows & 26310 unique IDs. Then in an effort to randomize things:\n",
    "# They divided those unique IDs into 15 chunks of 1754 (15 * 1754 = 26310)\n",
    "\n",
    "# The chunks could be either: Pairs\n",
    "# In that case we have 1754 / 2 = 877 pairs in each chunk (877 * 15 = 13155)\n",
    "# Or each chunk was made up of: Random IDs\n",
    "# In that case, they would have had to be very careful not to pair them with their real partners\n",
    "\n",
    "# They then took those 15 chunks and duplicated them in FirstId between 7 & 21 times\n",
    "# In case 1, they then took all IDs, removed the IDs which were in the chunk, and assigned them 'at random' to\n",
    "# each of those bigger chunks - e.g. Say chunk 1 was repeated 7 times, hence a total of 7*1754 entries\n",
    "# They then took the total of 26310 IDs - the 1754 IDs in that chunk and assigned them to those 7*1754 entires via \n",
    "# Second Id\n",
    "# I'm saying 'at random', as they seemed to change their approach of assigning SecondIDs throughout the process\n",
    "# First they used each SecondID for the same amount of times (1754) and then varied them uniformly e.g. to 1740 etc.\n",
    "\n",
    "# At the end they added another completely random ID to each chunk (IDs without a partner) \n",
    "# - changing the setup to 15 chunks of 1755\n",
    "\n",
    "\n",
    "# Find the unique combination to satisfy the constrain that each ID is only used once\n",
    "# In order to solve the problem let's turn it into a sparse matrix\n",
    "\n",
    "\n",
    "# Mistake in my assumption / understanding: An ID can be scored as 1 with multiple different IDs! \n",
    "# It's not about unique pairing of IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26325, 26325)\n",
      "368550.0\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix of shape [n_unique, n_unique]\n",
    "adjacency_matrix = np.zeros((26325, 26325))\n",
    "print(adjacency_matrix.shape)\n",
    "\n",
    "for idx, row in test.iterrows():\n",
    "    adjacency_matrix[row.FirstId, row.SecondId] += 1\n",
    "    \n",
    "print(np.sum(adjacency_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.sparse.csr_matrix((3, 4), dtype=np.int8).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2],\n",
       "       [0, 0, 3],\n",
       "       [4, 5, 6]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "scipy.sparse.csr_matrix((data, (row, col)), shape=(3, 3)).toarray()\n",
    "\n",
    "#csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n",
    "#where data, row_ind and col_ind satisfy the relationship a[row_ind[k], col_ind[k]] = data[k]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There seem to be distinct groups in SecondId\n",
      "{'1-5': '15,16', '10-14': '1755', '15-19': '1740'}\n"
     ]
    }
   ],
   "source": [
    "print('There seem to be distinct groups in SecondId')\n",
    "groups = {'1-5': '15,16', '10-14': '1755', '15-19': '1740'}\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many rows with same first & SecondId:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6 entries, 186443 to 327330\n",
      "Data columns (total 3 columns):\n",
      "pairId      6 non-null int64\n",
      "FirstId     6 non-null int64\n",
      "SecondId    6 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 192.0 bytes\n",
      "--------------------------------------------------\n",
      "Any duplicate rows?\n",
      "False    368550\n",
      "dtype: int64\n",
      "--------------------------------------------------\n",
      "Unique PairIDs: 368550\n",
      "--------------------------------------------------\n",
      "pairId      10000\n",
      "FirstId      8288\n",
      "SecondId     8265\n",
      "dtype: int64\n",
      "--------------------------------------------------\n",
      "Any inverse duplicate rows?\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 737100 entries, 0 to 368549\n",
      "Data columns (total 3 columns):\n",
      "pairId      737100 non-null int64\n",
      "FirstId     737100 non-null int64\n",
      "SecondId    737100 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 22.5 MB\n",
      "None\n",
      "Any duplicates, but swapped order?: False    737094\n",
      "True          6\n",
      "dtype: int64\n",
      "--------------------------------------------------\n",
      "Distribution of Ids\n",
      "0 26324\n",
      "0 26324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8c8d393da0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c8d393550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('How many rows with same first & SecondId:')\n",
    "test.loc[test.FirstId == test.SecondId].info()\n",
    "print('-'*50)\n",
    "print('Any duplicate rows?')\n",
    "print(test.duplicated().value_counts())\n",
    "\n",
    "# Any mistakes in pairId\n",
    "print('-'*50)\n",
    "print('Unique PairIDs:', test.pairId.nunique())\n",
    "\n",
    "# Unique values in first 10000 rows\n",
    "print('-'*50)\n",
    "print(test[:10000].nunique())\n",
    "\n",
    "print('-'*50)\n",
    "print('Any inverse duplicate rows?')\n",
    "id_inv_df = test[['pairId']].assign(FirstId = test.SecondId, SecondId = test.FirstId)\n",
    "combined_df = pd.concat([test, id_inv_df])\n",
    "print(combined_df.info())\n",
    "print('Any duplicates, but swapped order?:', combined_df.duplicated().value_counts())\n",
    "\n",
    "print('-'*50)\n",
    "print('Distribution of Ids')\n",
    "print(test.FirstId.min(), test.FirstId.max())\n",
    "print(test.SecondId.min(), test.SecondId.max())\n",
    "test.FirstId.plot.line()\n",
    "test.SecondId.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and leakage intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already know, the key to discover data leakages is careful EDA. So let's start our work with some basic data exploration and build an intuition about the leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check, how many different `id`s are there: concatenate `FirstId` and `SecondId` and print the number of unique elements. Also print minimum and maximum value for that vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in concat: 26325\n",
      "Min & Max ID: 0 26324\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "concat = pd.concat([test['FirstId'], test['SecondId']])\n",
    "print('Unique values in concat:', concat.nunique())\n",
    "print('Min & Max ID:', concat.min(), concat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then print how many pairs we need to classify (it is basically the number of rows in the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368550"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print, how many distinct pairs it would be possible to create out of all \"images\" in the dataset?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346489650"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "int((concat.nunique() * (concat.nunique() - 1) ) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the number of pairs we are given to classify is very very small compared to the total number of pairs. \n",
    "\n",
    "To exploit the leak we need to **assume (or prove)**, that the total number of positive pairs is small, compared to the total number of pairs. For example: think about an image dataset with $1000$ classes, $N$ images per class. Then if the task was to tell whether a pair of images belongs to the same class or not, we would have $1000\\frac{N(N-1)}{2}$ positive pairs, while total number of pairs was $\\frac{1000N(1000N - 1)}{2}$.\n",
    "\n",
    "Another example: in [Quora competitition](https://www.kaggle.com/c/quora-question-pairs) the task was to classify whether a pair of qustions are duplicates of each other or not. Of course, total number of question pairs is very huge, while number of duplicates (positive pairs) is much much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, let's get a fraction of pairs of class `1`. We just need to submit a constant prediction \"all ones\" and check the returned accuracy. Create a dataframe with columns `pairId` and `Prediction`, fill it and export it to `.csv` file. Then submit to grader and examine grader's output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "prediction_df = test[['pairId']].assign(Prediction=1)\n",
    "prediction_df.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we assumed the total number of pairs is much higher than the number of positive pairs, but it is not the case for the test set. It means that the test set is constructed not by sampling random pairs, but with a specific sampling algorithm. Pairs of class `1` are oversampled.\n",
    "\n",
    "Now think, how we can exploit this fact? What is the leak here? If you get it now, you may try to get to the final answer yourself, othewise you can follow the instructions below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a magic feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will build a magic feature, that will solve the problem almost perfectly. The instructions will lead you to the correct solution, but please, try to explain the purpose of the steps we do to yourself -- it is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incidence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to build an [incidence matrix](https://en.wikipedia.org/wiki/Incidence_matrix). You can think of pairs `(FirstId, SecondId)` as of edges in an undirected graph. \n",
    "\n",
    "The incidence matrix is a matrix of size `(maxId + 1, maxId + 1)`, where each row (column) `i` corresponds `i-th` `Id`. In this matrix we put the value `1` to the position `[i, j]`, if and only if a pair `(i, j)` or `(j, i)` is present in  a given set of pais `(FirstId, SecondId)`. All the other elements in the incidence matrix are zeros.   \n",
    "\n",
    "**Important!** The incidence matrices are typically very very sparse (small number of non-zero values). At the same time incidence matrices are usually huge in terms of total number of elements, and it is **impossible to store them in memory in dense format**. But due to their sparsity incidence matrices **can be easily represented as sparse matrices**. If you are not familiar with sparse matrices, please see [wiki](https://en.wikipedia.org/wiki/Sparse_matrix) and [scipy.sparse reference](https://docs.scipy.org/doc/scipy/reference/sparse.html). Please, use any of `scipy.sparse` constructors to build incidence matrix. \n",
    "\n",
    "For example, you can use this constructor: `scipy.sparse.coo_matrix((data, (i, j)))`. We highly recommend to learn to use different `scipy.sparse` constuctors, and matrices types, but if you feel you don't want to use them, you can always build this matrix with a simple `for` loop. You will need first to create a matrix using `scipy.sparse.coo_matrix((M, N), [dtype])` with an appropriate shape `(M, N)` and then iterate through `(FirstId, SecondId)` pairs and fill corresponding elements in matrix with ones. \n",
    "\n",
    "**Note**, that the matrix should be symmetric and consist only of zeros and ones. It is a way to check yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368550, 3) 737100\n",
      "(736872, 3)\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "print(test.shape, test.shape[0]*2)\n",
    "id_inv_df = test[['pairId']].assign(FirstId = test.SecondId, SecondId = test.FirstId)\n",
    "combined_df = pd.concat([test, id_inv_df])\n",
    "clean_test = combined_df.drop_duplicates(['FirstId','SecondId'])\n",
    "print(clean_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(736872,) (736872,)\n"
     ]
    }
   ],
   "source": [
    "# Rows:\n",
    "i = np.array(clean_test['FirstId'])\n",
    "# Cols:\n",
    "j = np.array(clean_test['SecondId'])\n",
    "print(i.shape, j.shape)\n",
    "data = np.ones((736872))\n",
    "\n",
    "inc_mat = scipy.sparse.coo_matrix((data, (i, j)))\n",
    "\n",
    "# YOUR CODE GOES HERE (but probably you will need to write few more lines before)\n",
    "\n",
    "# Sanity checks\n",
    "assert inc_mat.max() == 1\n",
    "assert inc_mat.sum() == 736872"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to have matrix in `csr` format eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compressed Row Format\n",
    "inc_mat = inc_mat.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now build the magic feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we build the incidence matrix? We can think of the rows in this matix as of representations for the objects. `i-th` row is a representation for an object with `Id = i`. Then, to measure similarity between two objects we can measure similarity between their representations. And we will see, that such representations are very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select the rows from the incidence matrix, that correspond to `test.FirstId`'s, and `test.SecondId`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368550, 26325) (368550, 26325)\n",
      "36.0\n",
      "36.0\n",
      "The first ID in FirstId was repeated 36.0  times.\n",
      "The first ID is:  1427\n",
      "--------------------------------------------------\n",
      "Sanity Check:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pairId      36\n",
       "FirstId     36\n",
       "SecondId    36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note, scipy goes crazy if a matrix is indexed with pandas' series. \n",
    "# So do not forget to convert `pd.series` to `np.array`\n",
    "# These lines should normally run very quickly \n",
    "\n",
    "rows_FirstId = inc_mat[np.array(test.FirstId)]\n",
    "# YOUR CODE GOES HERE\n",
    "rows_SecondId = inc_mat[np.array(test.SecondId)]\n",
    "# YOUR CODE GOES HERE\n",
    "print(rows_FirstId.shape, rows_SecondId.shape)\n",
    "print(rows_FirstId[0, :].sum())\n",
    "print(rows_SecondId[0, :].sum())\n",
    "print('The first ID in FirstId was repeated', np.sum(rows_FirstId[0, :]), ' times.')\n",
    "\n",
    "print('The first ID is: ', test.FirstId[0])\n",
    "\n",
    "print('-'*50)\n",
    "print('Sanity Check:')\n",
    "test.loc[test.FirstId == 1427].count() + test.loc[test.SecondId == 1427].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our magic feature will be the *dot product* between representations of a pair of objects. Dot product can be regarded as similarity measure -- for our non-negative representations the dot product is close to 0 when the representations are different, and is huge, when representations are similar. \n",
    "\n",
    "Now compute dot product between corresponding rows in `rows_FirstId` and `rows_SecondId` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368550, 26325) (368550, 26325)\n",
      "(368550,)\n",
      "[ 20.  14.  20.  20.  14.  20.  14.  14.  14.  14.  20.  20.  20.  14.  14.\n",
      "  14.  14.  14.  14.  20.  14.  20.  14.  20.  20.  14.  14.  14.  20.  20.\n",
      "  20.  14.  14.  20.  14.  14.  14.  14.  20.  14.  14.  20.  20.  20.  14.\n",
      "  14.  14.  20.  14.  14.]\n"
     ]
    }
   ],
   "source": [
    "# Note, that in order to do pointwise multiplication in scipy.sparse you need to use function `multiply`\n",
    "# regular `*` corresponds to matrix-matrix multiplication\n",
    "\n",
    "# Multiply the two IDs first, then take the sum  \n",
    "# (It's like taking the dot product just instead of row * column, row*row & we only multiply each row with the first)\n",
    "# We need to turn it back into an np array to be able to ravel it to the 1D shape\n",
    "print(rows_FirstId.shape, rows_SecondId.shape)\n",
    "f = np.array(np.sum(rows_FirstId.multiply(rows_SecondId), axis=1)).ravel()\n",
    "print(f.shape)\n",
    "print(f[:50])\n",
    "\n",
    "# Sanity check\n",
    "assert f.shape == (368550, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it! **We've built our magic feature.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From magic feature to binary predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we convert this feature into binary predictions? We do not have a train set to learn a model, but we have a piece of information about test set: the baseline accuracy score that you got, when submitting constant. And we also have a very strong considerations about the data generative process, so probably we will be fine even without a training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may try to choose a thresold, and set the predictions to 1, if the feature value `f` is higer than the threshold, and 0 otherwise. What threshold would you choose? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find a right threshold? Let's first examine this feature: print frequencies (or counts) of each value in the feature `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 14.,  15.,  19.,  20.,  21.,  28.,  35.]), array([183279,    852,    546, 183799,      6,     54,     14]))\n"
     ]
    }
   ],
   "source": [
    "# For example use `np.unique` function, check for flags\n",
    "\n",
    "print(np.unique(f, return_counts=True))\n",
    "# -> Let's pick 18 as the threshold\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see how this feature clusters the pairs? Maybe you can guess a good threshold by looking at the values? \n",
    "\n",
    "In fact, in other situations it can be not that obvious, but in general to pick a threshold you only need to remember the score of your baseline submission and use this information. Do you understand why and how?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a threshold below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = f > 18 # SET THRESHOLD HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, let's create a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = test.loc[:,['pairId']]\n",
    "submission['Prediction'] = pred.astype(int)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feedback: Well done! Your accuracy score is 0.999609 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now submit it to the grader! It is not possible to submit directly from this notebook, as we need to submit a `csv` file, not a single number (limitation of Coursera platform). \n",
    "\n",
    "To download `submission.csv` file that you've just produced <a href='./submission.csv'>click here</a> (if the link opens in browser, right-click on it and shoose \"Save link as\"). Then go to [assignment page](https://www.coursera.org/learn/competitive-data-science/programming/KsASv/data-leakages/submission) and submit your `.csv` file in 'My submission' tab.\n",
    "\n",
    "\n",
    "If you did everything right, the score should be very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally:** try to explain to yourself, why the whole thing worked out. In fact, there is no magic in this feature, and the idea to use rows in the incidence matrix can be intuitively justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, it is not the only leak in this dataset. There is another totally different way to get almost 100% accuracy. Try to find it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
